{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predixion.ai Assignment\n",
        "\n",
        "1.   Summarization\n",
        "2.   Action Words\n",
        "2.   Sentiment Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "EE4PZyAl-lBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Libraries"
      ],
      "metadata": {
        "id": "Fg_7VS5a--R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n",
        "!pip install pandas\n",
        "!pip install googletrans==3.1.0a0\n",
        "!pip install nltk sumy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtv9vDfw-1K7",
        "outputId": "270bbd64-a39c-45ec-dcfb-79e9c91fa6ba",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m92.2/126.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.7.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.7.4)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hstspreload-2024.7.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16353 sha256=9a7c9e963c6ac7452b7fdf71350656c8e1e5fcc4cfadb396920dfdb498c93c69\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.7.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.7.4)\n",
            "Building wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=9ef3631f16e6a1db56c185da129b26ebe7ad8870a23e7e198d9151f68124338c\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=90d88d775bb51afa228cf3d86e3aad62812ac3d02bae8d4605b2eec0269729d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "# from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "59H-owmb2pt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d278050-7375-4518-b1dd-a217a29aefe1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coversatation Text\n"
      ],
      "metadata": {
        "id": "1SwOmjjD_EoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_text = \"\"\"\n",
        "Recovery Agent (RA): नमस्ते श्री कुमार, मैं एक्स वाई जेड फाइनेंस से बोल रहा हूं। आपके लोन के बारे में बात करनी थी।\n",
        "Borrower (B): हां, बोलिए। क्या बात है?\n",
        "RA: सर, आपका पिछले महीने का EMI अभी तक नहीं आया है। क्या कोई समस्या है?\n",
        "B: हां, थोड़ी दिक्कत है। मेरी नौकरी चली गई है और मैं नया काम ढूंढ रहा हूं।\n",
        "RA: ओह, यह तो बुरा हुआ। लेकिन सर, आपको समझना होगा कि लोन का भुगतान समय पर करना बहुत जरूरी है।\n",
        "B: मैं समझता हूं, लेकिन अभी मेरे पास पैसे नहीं हैं। क्या कुछ समय मिल सकता है?\n",
        "RA: हम समझते हैं आपकी स्थिति। क्या आप अगले हफ्ते तक कुछ भुगतान कर सकते हैं?\n",
        "B: मैं कोशिश करूंगा, लेकिन पूरा EMI नहीं दे पाऊंगा। क्या आधा भुगतान चलेगा?\n",
        "RA: ठीक है, आधा भुगतान अगले हफ्ते तक कर दीजिए। बाकी का क्या प्लान है आपका?\n",
        "B: मुझे उम्मीद है कि अगले महीने तक मुझे नया काम मिल जाएगा। तब मैं बाकी बकाया चुका दूंगा।\n",
        "RA: ठीक है। तो हम ऐसा करते हैं - आप अगले हफ्ते तक आधा EMI जमा कर दीजिए, और अगले महीने के 15 तारीख तक बाकी का भुगतान कर दीजिए। क्या यह आपको स्वीकार है?\n",
        "B: हां, यह ठीक रहेगा। मैं इस प्लान का पालन करने की पूरी कोशिश करूंगा।\n",
        "RA: बहुत अच्छा। मैं आपको एक SMS भेज रहा हूं जिसमें भुगतान की डिटेल्स होंगी। कृपया इसका पालन करें और समय पर भुगतान करें।\n",
        "B: ठीक है, धन्यवाद आपके समझने के लिए।\n",
        "RA: आपका स्वागत है। अगर कोई और सवाल हो तो मुझे बताइएगा। अलविदा।\n",
        "B: अलविदा।\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bgTd-xQJBs0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting text into proper format"
      ],
      "metadata": {
        "id": "ScoMKH-R9E12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = conversation_text.strip().split('\\n')\n",
        "\n",
        "# Initialize the conversation list\n",
        "conversation = []\n",
        "\n",
        "# Iterate through each line and create a dictionary for each dialogue\n",
        "for line in lines:\n",
        "    if \": \" in line:\n",
        "        speaker, dialogue = line.split(\": \", 1)\n",
        "        speaker = speaker.strip()\n",
        "        dialogue = dialogue.strip()\n",
        "        # Append the dictionary to the conversation list\n",
        "        conversation.append({\"speaker\": speaker, \"line\": dialogue})\n",
        "\n",
        "# Print the formatted conversation\n",
        "for entry in conversation:\n",
        "    print(entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16AggnM-Bkwc",
        "outputId": "e2398196-92a9-4dba-b0c6-aead45550ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'speaker': 'Recovery Agent (RA)', 'line': 'नमस्ते श्री कुमार, मैं एक्स वाई जेड फाइनेंस से बोल रहा हूं। आपके लोन के बारे में बात करनी थी।'}\n",
            "{'speaker': 'Borrower (B)', 'line': 'हां, बोलिए। क्या बात है?'}\n",
            "{'speaker': 'RA', 'line': 'सर, आपका पिछले महीने का EMI अभी तक नहीं आया है। क्या कोई समस्या है?'}\n",
            "{'speaker': 'B', 'line': 'हां, थोड़ी दिक्कत है। मेरी नौकरी चली गई है और मैं नया काम ढूंढ रहा हूं।'}\n",
            "{'speaker': 'RA', 'line': 'ओह, यह तो बुरा हुआ। लेकिन सर, आपको समझना होगा कि लोन का भुगतान समय पर करना बहुत जरूरी है।'}\n",
            "{'speaker': 'B', 'line': 'मैं समझता हूं, लेकिन अभी मेरे पास पैसे नहीं हैं। क्या कुछ समय मिल सकता है?'}\n",
            "{'speaker': 'RA', 'line': 'हम समझते हैं आपकी स्थिति। क्या आप अगले हफ्ते तक कुछ भुगतान कर सकते हैं?'}\n",
            "{'speaker': 'B', 'line': 'मैं कोशिश करूंगा, लेकिन पूरा EMI नहीं दे पाऊंगा। क्या आधा भुगतान चलेगा?'}\n",
            "{'speaker': 'RA', 'line': 'ठीक है, आधा भुगतान अगले हफ्ते तक कर दीजिए। बाकी का क्या प्लान है आपका?'}\n",
            "{'speaker': 'B', 'line': 'मुझे उम्मीद है कि अगले महीने तक मुझे नया काम मिल जाएगा। तब मैं बाकी बकाया चुका दूंगा।'}\n",
            "{'speaker': 'RA', 'line': 'ठीक है। तो हम ऐसा करते हैं - आप अगले हफ्ते तक आधा EMI जमा कर दीजिए, और अगले महीने के 15 तारीख तक बाकी का भुगतान कर दीजिए। क्या यह आपको स्वीकार है?'}\n",
            "{'speaker': 'B', 'line': 'हां, यह ठीक रहेगा। मैं इस प्लान का पालन करने की पूरी कोशिश करूंगा।'}\n",
            "{'speaker': 'RA', 'line': 'बहुत अच्छा। मैं आपको एक SMS भेज रहा हूं जिसमें भुगतान की डिटेल्स होंगी। कृपया इसका पालन करें और समय पर भुगतान करें।'}\n",
            "{'speaker': 'B', 'line': 'ठीक है, धन्यवाद आपके समझने के लिए।'}\n",
            "{'speaker': 'RA', 'line': 'आपका स्वागत है। अगर कोई और सवाल हो तो मुझे बताइएगा। अलविदा।'}\n",
            "{'speaker': 'B', 'line': 'अलविदा।'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translating text into english as most libraries not support hindi"
      ],
      "metadata": {
        "id": "vwtbRnKz9LJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qapRWUCw9K3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "translated_conversation = []\n",
        "for entry in conversation:\n",
        "    translated_text = translator.translate(entry['line'], src='hi', dest='en').text\n",
        "    translated_conversation.append({'speaker': entry['speaker'], 'line': translated_text})"
      ],
      "metadata": {
        "id": "z6nN6xulB5dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Summary ***\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eFyWaEtC9iFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "UkyAmH1N2pj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee1119f-3a92-4340-cd29-efc3a36584d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the translated lines into a single paragraph\n",
        "paragraph_english = ' '.join(entry['line'] for entry in translated_conversation)\n",
        "print(\"Translated Paragraph:\", paragraph_english)\n"
      ],
      "metadata": {
        "id": "LYl-9Df-2pTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd59d120-9ec0-48ae-87bd-283fdd9e0f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Paragraph: Hello Mr. Kumar, I am speaking from XYZ Finance. Wanted to talk about your loan. Yes, tell me. What is the matter? Sir, your last month's EMI has not arrived yet. are there any problems? Yes, there is a little problem. I have lost my job and am looking for a new job. Oh, this was bad. But sir, you have to understand that it is very important to repay the loan on time. I understand, but I don't have money right now. Can I get some time? We understand your situation. Can you make some payments by next week? I will try, but will not be able to pay the full EMI. Will half payment suffice? Okay, pay half by next week. What is your plan for the rest? I hope I will get a new job by next month. Then I will pay the remaining dues. Ok. So we do this - you deposit half the EMI by the next week, and pay the rest by the 15th of the next month. Is this acceptable to you? Yes, that will be fine. I will try my best to follow this plan. Very good. I am sending you an SMS containing payment details. Please follow this and make payment on time. Okay, thank you for your understanding. you are welcome. If you have any other questions let me know. goodbye. goodbye.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Stopword Removal"
      ],
      "metadata": {
        "id": "NRatp6br8vK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tokenization\n",
        "tokens = word_tokenize(paragraph_english)\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcfDS5ukJdGX",
        "outputId": "ffb1947e-756f-4733-b278-23f891f66773"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Hello', 'Mr.', 'Kumar', ',', 'I', 'am', 'speaking', 'from', 'XYZ', 'Finance', '.', 'Wanted', 'to', 'talk', 'about', 'your', 'loan', '.', 'Yes', ',', 'tell', 'me', '.', 'What', 'is', 'the', 'matter', '?', 'Sir', ',', 'your', 'last', 'month', \"'s\", 'EMI', 'has', 'not', 'arrived', 'yet', '.', 'are', 'there', 'any', 'problems', '?', 'Yes', ',', 'there', 'is', 'a', 'little', 'problem', '.', 'I', 'have', 'lost', 'my', 'job', 'and', 'am', 'looking', 'for', 'a', 'new', 'job', '.', 'Oh', ',', 'this', 'was', 'bad', '.', 'But', 'sir', ',', 'you', 'have', 'to', 'understand', 'that', 'it', 'is', 'very', 'important', 'to', 'repay', 'the', 'loan', 'on', 'time', '.', 'I', 'understand', ',', 'but', 'I', 'do', \"n't\", 'have', 'money', 'right', 'now', '.', 'Can', 'I', 'get', 'some', 'time', '?', 'We', 'understand', 'your', 'situation', '.', 'Can', 'you', 'make', 'some', 'payments', 'by', 'next', 'week', '?', 'I', 'will', 'try', ',', 'but', 'will', 'not', 'be', 'able', 'to', 'pay', 'the', 'full', 'EMI', '.', 'Will', 'half', 'payment', 'suffice', '?', 'Okay', ',', 'pay', 'half', 'by', 'next', 'week', '.', 'What', 'is', 'your', 'plan', 'for', 'the', 'rest', '?', 'I', 'hope', 'I', 'will', 'get', 'a', 'new', 'job', 'by', 'next', 'month', '.', 'Then', 'I', 'will', 'pay', 'the', 'remaining', 'dues', '.', 'Ok', '.', 'So', 'we', 'do', 'this', '-', 'you', 'deposit', 'half', 'the', 'EMI', 'by', 'the', 'next', 'week', ',', 'and', 'pay', 'the', 'rest', 'by', 'the', '15th', 'of', 'the', 'next', 'month', '.', 'Is', 'this', 'acceptable', 'to', 'you', '?', 'Yes', ',', 'that', 'will', 'be', 'fine', '.', 'I', 'will', 'try', 'my', 'best', 'to', 'follow', 'this', 'plan', '.', 'Very', 'good', '.', 'I', 'am', 'sending', 'you', 'an', 'SMS', 'containing', 'payment', 'details', '.', 'Please', 'follow', 'this', 'and', 'make', 'payment', 'on', 'time', '.', 'Okay', ',', 'thank', 'you', 'for', 'your', 'understanding', '.', 'you', 'are', 'welcome', '.', 'If', 'you', 'have', 'any', 'other', 'questions', 'let', 'me', 'know', '.', 'goodbye', '.', 'goodbye', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered Tokens:\", filtered_tokens)"
      ],
      "metadata": {
        "id": "JLa87Z_T2o-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e4edd3-6b09-483b-e281-a91e680d8b9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Hello', 'Mr.', 'Kumar', ',', 'speaking', 'XYZ', 'Finance', '.', 'Wanted', 'talk', 'loan', '.', 'Yes', ',', 'tell', '.', 'matter', '?', 'Sir', ',', 'last', 'month', \"'s\", 'EMI', 'arrived', 'yet', '.', 'problems', '?', 'Yes', ',', 'little', 'problem', '.', 'lost', 'job', 'looking', 'new', 'job', '.', 'Oh', ',', 'bad', '.', 'sir', ',', 'understand', 'important', 'repay', 'loan', 'time', '.', 'understand', ',', \"n't\", 'money', 'right', '.', 'get', 'time', '?', 'understand', 'situation', '.', 'make', 'payments', 'next', 'week', '?', 'try', ',', 'able', 'pay', 'full', 'EMI', '.', 'half', 'payment', 'suffice', '?', 'Okay', ',', 'pay', 'half', 'next', 'week', '.', 'plan', 'rest', '?', 'hope', 'get', 'new', 'job', 'next', 'month', '.', 'pay', 'remaining', 'dues', '.', 'Ok', '.', '-', 'deposit', 'half', 'EMI', 'next', 'week', ',', 'pay', 'rest', '15th', 'next', 'month', '.', 'acceptable', '?', 'Yes', ',', 'fine', '.', 'try', 'best', 'follow', 'plan', '.', 'good', '.', 'sending', 'SMS', 'containing', 'payment', 'details', '.', 'Please', 'follow', 'make', 'payment', 'time', '.', 'Okay', ',', 'thank', 'understanding', '.', 'welcome', '.', 'questions', 'let', 'know', '.', 'goodbye', '.', 'goodbye', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Stemming/Lemmatization"
      ],
      "metadata": {
        "id": "rLGIgZa580_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming/Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "\n",
        "# Combine tokens back to text\n",
        "processed_text = ' '.join(lemmatized_tokens)\n",
        "print(\"Processed Text:\", processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuLRXGYB6UAe",
        "outputId": "2f0fe21f-8939-4872-ae71-70e90897a5e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens: ['hello', 'mr.', 'kumar', ',', 'speak', 'xyz', 'financ', '.', 'want', 'talk', 'loan', '.', 'ye', ',', 'tell', '.', 'matter', '?', 'sir', ',', 'last', 'month', \"'s\", 'emi', 'arriv', 'yet', '.', 'problem', '?', 'ye', ',', 'littl', 'problem', '.', 'lost', 'job', 'look', 'new', 'job', '.', 'oh', ',', 'bad', '.', 'sir', ',', 'understand', 'import', 'repay', 'loan', 'time', '.', 'understand', ',', \"n't\", 'money', 'right', '.', 'get', 'time', '?', 'understand', 'situat', '.', 'make', 'payment', 'next', 'week', '?', 'tri', ',', 'abl', 'pay', 'full', 'emi', '.', 'half', 'payment', 'suffic', '?', 'okay', ',', 'pay', 'half', 'next', 'week', '.', 'plan', 'rest', '?', 'hope', 'get', 'new', 'job', 'next', 'month', '.', 'pay', 'remain', 'due', '.', 'ok', '.', '-', 'deposit', 'half', 'emi', 'next', 'week', ',', 'pay', 'rest', '15th', 'next', 'month', '.', 'accept', '?', 'ye', ',', 'fine', '.', 'tri', 'best', 'follow', 'plan', '.', 'good', '.', 'send', 'sm', 'contain', 'payment', 'detail', '.', 'pleas', 'follow', 'make', 'payment', 'time', '.', 'okay', ',', 'thank', 'understand', '.', 'welcom', '.', 'question', 'let', 'know', '.', 'goodby', '.', 'goodby', '.']\n",
            "Lemmatized Tokens: ['Hello', 'Mr.', 'Kumar', ',', 'speaking', 'XYZ', 'Finance', '.', 'Wanted', 'talk', 'loan', '.', 'Yes', ',', 'tell', '.', 'matter', '?', 'Sir', ',', 'last', 'month', \"'s\", 'EMI', 'arrived', 'yet', '.', 'problem', '?', 'Yes', ',', 'little', 'problem', '.', 'lost', 'job', 'looking', 'new', 'job', '.', 'Oh', ',', 'bad', '.', 'sir', ',', 'understand', 'important', 'repay', 'loan', 'time', '.', 'understand', ',', \"n't\", 'money', 'right', '.', 'get', 'time', '?', 'understand', 'situation', '.', 'make', 'payment', 'next', 'week', '?', 'try', ',', 'able', 'pay', 'full', 'EMI', '.', 'half', 'payment', 'suffice', '?', 'Okay', ',', 'pay', 'half', 'next', 'week', '.', 'plan', 'rest', '?', 'hope', 'get', 'new', 'job', 'next', 'month', '.', 'pay', 'remaining', 'due', '.', 'Ok', '.', '-', 'deposit', 'half', 'EMI', 'next', 'week', ',', 'pay', 'rest', '15th', 'next', 'month', '.', 'acceptable', '?', 'Yes', ',', 'fine', '.', 'try', 'best', 'follow', 'plan', '.', 'good', '.', 'sending', 'SMS', 'containing', 'payment', 'detail', '.', 'Please', 'follow', 'make', 'payment', 'time', '.', 'Okay', ',', 'thank', 'understanding', '.', 'welcome', '.', 'question', 'let', 'know', '.', 'goodbye', '.', 'goodbye', '.']\n",
            "Processed Text: Hello Mr. Kumar , speaking XYZ Finance . Wanted talk loan . Yes , tell . matter ? Sir , last month 's EMI arrived yet . problem ? Yes , little problem . lost job looking new job . Oh , bad . sir , understand important repay loan time . understand , n't money right . get time ? understand situation . make payment next week ? try , able pay full EMI . half payment suffice ? Okay , pay half next week . plan rest ? hope get new job next month . pay remaining due . Ok . - deposit half EMI next week , pay rest 15th next month . acceptable ? Yes , fine . try best follow plan . good . sending SMS containing payment detail . Please follow make payment time . Okay , thank understanding . welcome . question let know . goodbye . goodbye .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Key Action Words"
      ],
      "metadata": {
        "id": "cyr465XiFJ5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(lemmatized_tokens)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "id": "DaLz2jS6AZjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fc30bc-349b-4f57-bb07-035a52175a16"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Hello', 'NNP'), ('Mr.', 'NNP'), ('Kumar', 'NNP'), (',', ','), ('speaking', 'VBG'), ('XYZ', 'NNP'), ('Finance', 'NNP'), ('.', '.'), ('Wanted', 'VBD'), ('talk', 'NN'), ('loan', 'NN'), ('.', '.'), ('Yes', 'UH'), (',', ','), ('tell', 'NN'), ('.', '.'), ('matter', 'NN'), ('?', '.'), ('Sir', 'NNP'), (',', ','), ('last', 'JJ'), ('month', 'NN'), (\"'s\", 'POS'), ('EMI', 'NNP'), ('arrived', 'VBD'), ('yet', 'RB'), ('.', '.'), ('problem', 'NN'), ('?', '.'), ('Yes', 'UH'), (',', ','), ('little', 'JJ'), ('problem', 'NN'), ('.', '.'), ('lost', 'VBN'), ('job', 'NN'), ('looking', 'VBG'), ('new', 'JJ'), ('job', 'NN'), ('.', '.'), ('Oh', 'UH'), (',', ','), ('bad', 'JJ'), ('.', '.'), ('sir', 'NN'), (',', ','), ('understand', 'VBP'), ('important', 'JJ'), ('repay', 'NN'), ('loan', 'NN'), ('time', 'NN'), ('.', '.'), ('understand', 'NN'), (',', ','), (\"n't\", 'RB'), ('money', 'NN'), ('right', 'NN'), ('.', '.'), ('get', 'VB'), ('time', 'NN'), ('?', '.'), ('understand', 'JJ'), ('situation', 'NN'), ('.', '.'), ('make', 'VB'), ('payment', 'NN'), ('next', 'IN'), ('week', 'NN'), ('?', '.'), ('try', 'NN'), (',', ','), ('able', 'JJ'), ('pay', 'NN'), ('full', 'JJ'), ('EMI', 'NNP'), ('.', '.'), ('half', 'JJ'), ('payment', 'NN'), ('suffice', 'NN'), ('?', '.'), ('Okay', 'NNP'), (',', ','), ('pay', 'NN'), ('half', 'NN'), ('next', 'JJ'), ('week', 'NN'), ('.', '.'), ('plan', 'NN'), ('rest', 'VB'), ('?', '.'), ('hope', 'VB'), ('get', 'VB'), ('new', 'JJ'), ('job', 'NN'), ('next', 'JJ'), ('month', 'NN'), ('.', '.'), ('pay', 'NN'), ('remaining', 'VBG'), ('due', 'JJ'), ('.', '.'), ('Ok', 'NNP'), ('.', '.'), ('-', ':'), ('deposit', 'NN'), ('half', 'NN'), ('EMI', 'NNP'), ('next', 'JJ'), ('week', 'NN'), (',', ','), ('pay', 'VB'), ('rest', 'JJS'), ('15th', 'CD'), ('next', 'JJ'), ('month', 'NN'), ('.', '.'), ('acceptable', 'JJ'), ('?', '.'), ('Yes', 'UH'), (',', ','), ('fine', 'JJ'), ('.', '.'), ('try', 'NN'), ('best', 'JJS'), ('follow', 'JJ'), ('plan', 'NN'), ('.', '.'), ('good', 'JJ'), ('.', '.'), ('sending', 'VBG'), ('SMS', 'NNP'), ('containing', 'VBG'), ('payment', 'NN'), ('detail', 'NN'), ('.', '.'), ('Please', 'NNP'), ('follow', 'JJ'), ('make', 'VBP'), ('payment', 'NN'), ('time', 'NN'), ('.', '.'), ('Okay', 'NNP'), (',', ','), ('thank', 'VBD'), ('understanding', 'JJ'), ('.', '.'), ('welcome', 'JJ'), ('.', '.'), ('question', 'NN'), ('let', 'NN'), ('know', 'VB'), ('.', '.'), ('goodbye', 'NN'), ('.', '.'), ('goodbye', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_words = [word for word, pos in pos_tags if pos.startswith('VB')]\n",
        "print(\"Action Words:\", action_words)"
      ],
      "metadata": {
        "id": "izqq-ViTA7Do",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "defe1a4f-5e65-491e-f54b-0c704157eaeb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Words: ['speaking', 'Wanted', 'arrived', 'lost', 'looking', 'understand', 'get', 'make', 'rest', 'hope', 'get', 'remaining', 'pay', 'sending', 'containing', 'make', 'thank', 'know']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Summarization"
      ],
      "metadata": {
        "id": "ZYr82Yp483GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Summarization\n",
        "parser = PlaintextParser.from_string(processed_text, Tokenizer(\"english\"))\n",
        "summarizer = LexRankSummarizer()\n",
        "summary = summarizer(parser.document, 4)  # Summarize to 5 sentences\n",
        "\n",
        "print(\"Summary:\")\n",
        "for sentence in summary:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "YA5aWf_C6fSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a79e5bf-24e1-41bb-87f0-2861b4dc7f8b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "sir , understand important repay loan time .\n",
            "Okay , pay half next week .\n",
            "hope get new job next month .\n",
            "- deposit half EMI next week , pay rest 15th next month .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SENTIMENT ANALYSIS  ***"
      ],
      "metadata": {
        "id": "bDKFurKp2Wig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jdnVDTBaB1R8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6Iaw_wQm_PC0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform sentiment analysis for each line"
      ],
      "metadata": {
        "id": "McqmHsvSEnYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results = []\n",
        "for entry in  translated_conversation:\n",
        "    sentiment = analyzer.polarity_scores(entry[\"line\"])\n",
        "    results.append({\n",
        "        \"Speaker\": entry[\"speaker\"],\n",
        "        \"Line\": entry[\"line\"],\n",
        "        \"Compound\": sentiment[\"compound\"],\n",
        "        \"Positive\": sentiment[\"pos\"],\n",
        "        \"Neutral\": sentiment[\"neu\"],\n",
        "        \"Negative\": sentiment[\"neg\"]\n",
        "    })\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "U5K6tlfF1rqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5931d437-93fb-4c55-fb3b-a29829b27a36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Speaker                                               Line  \\\n",
            "0   Recovery Agent (RA)  Hello Mr. Kumar, I am speaking from XYZ Financ...   \n",
            "1          Borrower (B)                  Yes, tell me. What is the matter?   \n",
            "2                    RA  Sir, your last month's EMI has not arrived yet...   \n",
            "3                     B  Yes, there is a little problem. I have lost my...   \n",
            "4                    RA  Oh, this was bad. But sir, you have to underst...   \n",
            "5                     B  I understand, but I don't have money right now...   \n",
            "6                    RA  We understand your situation. Can you make som...   \n",
            "7                     B  I will try, but will not be able to pay the fu...   \n",
            "8                    RA  Okay, pay half by next week. What is your plan...   \n",
            "9                     B  I hope I will get a new job by next month. The...   \n",
            "10                   RA  Ok. So we do this - you deposit half the EMI b...   \n",
            "11                    B  Yes, that will be fine. I will try my best to ...   \n",
            "12                   RA  Very good. I am sending you an SMS containing ...   \n",
            "13                    B            Okay, thank you for your understanding.   \n",
            "14                   RA  you are welcome. If you have any other questio...   \n",
            "15                    B                                           goodbye.   \n",
            "\n",
            "    Compound  Positive  Neutral  Negative  \n",
            "0     0.0000     0.000    1.000     0.000  \n",
            "1     0.4215     0.432    0.568     0.000  \n",
            "2    -0.4019     0.000    0.816     0.184  \n",
            "3    -0.2516     0.120    0.669     0.210  \n",
            "4     0.1001     0.110    0.795     0.094  \n",
            "5     0.0000     0.000    1.000     0.000  \n",
            "6     0.0000     0.000    1.000     0.000  \n",
            "7    -0.1531     0.000    0.909     0.091  \n",
            "8     0.1280     0.133    0.769     0.098  \n",
            "9     0.3612     0.143    0.788     0.069  \n",
            "10    0.3016     0.079    0.878     0.042  \n",
            "11    0.8271     0.442    0.558     0.000  \n",
            "12    0.6697     0.244    0.756     0.000  \n",
            "13    0.5267     0.524    0.476     0.000  \n",
            "14    0.4588     0.200    0.800     0.000  \n",
            "15    0.0000     0.000    1.000     0.000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " Separate sentiment analysis by speaker"
      ],
      "metadata": {
        "id": "AxUlQ4oXEuxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate sentiment analysis by speaker\n",
        "ra_sentiments = df[df[\"Speaker\"] == \"RA\"]\n",
        "b_sentiments = df[df[\"Speaker\"] == \"B\"]"
      ],
      "metadata": {
        "id": "p486hwMg1rQu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the average sentiment scores for each speaker\n",
        "ra_summary = ra_sentiments.mean(numeric_only=True)\n",
        "b_summary = b_sentiments.mean(numeric_only=True)\n"
      ],
      "metadata": {
        "id": "hTprFuXj156y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the summary results\n",
        "print(\"\\nRecovery Agent Sentiment Summary:\")\n",
        "print(ra_summary)\n",
        "\n",
        "print(\"\\nBorrower Sentiment Summary:\")\n",
        "print(b_summary)"
      ],
      "metadata": {
        "id": "GpOYp0eJ18-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3f53bf-3787-45dd-b461-86959bea0b53"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recovery Agent Sentiment Summary:\n",
            "Compound    0.179471\n",
            "Positive    0.109429\n",
            "Neutral     0.830571\n",
            "Negative    0.059714\n",
            "dtype: float64\n",
            "\n",
            "Borrower Sentiment Summary:\n",
            "Compound    0.187186\n",
            "Positive    0.175571\n",
            "Neutral     0.771429\n",
            "Negative    0.052857\n",
            "dtype: float64\n"
          ]
        }
      ]
    }
  ]
}